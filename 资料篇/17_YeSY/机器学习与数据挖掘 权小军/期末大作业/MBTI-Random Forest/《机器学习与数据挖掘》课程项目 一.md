# 《机器学习与数据挖掘》课程项目--Bagging

### 一、实验环境

本次实验是在windows10操作系统下进行的。使用的IDE为pycharm。

主要使用的python工具包列举如下：

```
xgboost
sklearn
pandas
numpy
re
```



### 二、实验目的

本次实验使用的是`kaggle`开源数据集 `Myers-Briggs Personality Type Dataset(MBTI)`。这个数据集给了一共8600条8种人格标签和样本的posts文本数据，我们需要尝试用posts的文本内容对4组人格进行分类：

1. Introversion (I) – Extroversion (E)
2. Intuition (N) – Sensing (S)
3. Thinking (T) – Feeling (F)
4. Judging (J) – Perceiving (P)

他们使用字符的组合来表征某一个人格特征的组合，比如`INTJ`表示这个人具备`Introversion, Intuition,Thinking,Judging   `的性格特征



### 三、实验原理

#### 1. TFIDF

TFIDF是一种对文本进行编码的方法，在自然语言处理中我们需要将文本转换成向量矩阵的表示，才能计算文本之间的相关度或者做预测或者分类。

onehot是一种编码的方法，它先遍历所有的文本收集词语，并根据根据每个词语是否出某一篇文章中，出现的话对应向量位置为1，否则为0，但这样就忽略了词语出现的频次所带来的影响，所以就进行了改进。TF方法是通过计算这个词语在这个句子中出现的次数归一化的结果填入词向量对应的项中，公式如下：
$$
tf_{i,j}=\frac{n_{k,j}}{\sum_kn_{k,j}}
$$
但是还是存在一些问题，因为tf的方法容易收到一些常见词语的影响，比如一些停用词语，如：的，吗，还有一些常见但包含的能代表文本的信息少的词语，比如：转发，评论，赞这种，他们数量很多，可能会影响到一些文本关键主题的突出。所以，又引入了一个概念叫做IDF也就是反文档频率，如果这个词语在每篇文章都出现过，说明它能反应文本特性的能力不足，应该缩小它在词语向量中的权重。我们可以计算词语的反文档频率，公式如下：
$$
idf_i=log\frac{|D|}{|\{j:t_i\in d_j\}|}
$$
然后我们就可以将反文档频率乘到每个词语的tf上，就可以得到TF-IDF矩阵的结果
$$
tfidf_{i,j} = tf_{i,j} \times idf_i
$$

#### 2. 随机森林

##### 模型描述

随机森林是很多决策树组成的分类模型。决策树是一种很简单的分类算法，它的解释性强，符合人类的直观思维。而随机森林简单的来讲就是由很多的决策树构成，不同的决策树之间是没有关联的。当我们进行分类任务的时候，新的样本输入进来，我们就让我们森林中的每一棵树各自进行分类和判断，每个决策树都会有自己的一个分类结果，最后决策树的分类结果中哪一个分类最多，那么随机森林就会把这个结果当成最终的结果。

![image-20200609203151559](assets\《机器学习与数据挖掘》课程项目 一\image-20200609203151559.png)

随机森林和单一的决策树模型的构造不完全相同。构造随机森林分为以下四个主要步骤：

1. 假如有N个样本，则**有放回的随机选择N个样本**(每次随机选择一个样本，然后返回继续选择)。这选择好了的N个样本用来训练一个决策树，作为决策树根节点处的样本。
2. 当每个样本有$M$个属性时，在决策树的每个节点需要分裂时，随机从这$M$个属性中选取出$m$个属性，满足条件$m << M$。然后从这$m$个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。
3. 决策树形成过程中每个节点都要按照步骤2来分裂。一直到不能够再分裂为止。整个随机森林的决策树形成过程中没有进行剪枝。而我们在做决策树与的时候，为了防止过拟合，往往会进行剪枝处理。
4. 按照步骤1~3建立大量的决策树，这样就构成了随机森林了。

##### 模型优点

1. 随机森林可以对很高维度的数据进行分类处理，不需要做特征选择或者PCA的降维处理
2. 决策树的信息熵或基尼系数等可以判断特征的重要程度
3. 随机森林不容易发生过拟合问题
4. 随机森林实现比较简单，并且因为每一个决策树是独立运行，可以做成并行的模型，提高运行效率。



### 四、实验过程和结论

#### 1. 数据预处理

我们首先对数据进行观察：每一行是一个样本，第一列是他们的人格分类类型，也就是`INTJ`之类的数据，第二列是他们的posts文本内容，这些文本内容可能有多条，用`|||`的符合间隔开来。同时数据里夹杂了很多链接，这些链接需要被我们去除。

```
'http://www.youtube.com/watch?v=qsXHcwe3krw",|||
  'http://41.media.tumblr.com/tumblr_lfouy03PMA1qa1rooo1_500.jpg',|||
  'enfp and intj moments  https://www.youtube.com/watch?v=iz7lE1g4XM4  sportscenter not top ten plays  https://www.youtube.com/watch?v=uCdfze1etec  pranks',|||
  'What has been the most life-changing experience in your life?',|||
  'http://www.youtube.com/watch?v=vXZeYwwRDw8   http://www.youtube.com/watch?v=u8ejam5DP3E  On repeat for most of today.',|||
  ...
```

同时我们进一步观察数据，会发现里面包含第一列人格类型的关键词，比如这一条posts：

```
Mm, probably INTP now that I have become more familiar with that type.
```

上面的句子中就包含了`INTP`的词语。这些词语对我们的分类肯定存在影响，因为这并不是他们个人的语言表达，而且这个单词和人格标签有很强的绑定关系，会影响我们对文本单词和人格类型潜在的相关性之间的关系，因此我们也需要对这些人格类型数据进行过滤。

```python
def change_personality(personality):
    # transform mbti to binary vector
    b_Pers = {'I': '0', 'E': '1', 'N': '0', 'S': '1', 'F': '0', 'T': '1', 'J': '0', 'P': '1'}
    return [b_Pers[l] for l in personality]

def pre_process_data(data):
    # 获取英文停用词列表
    cachedStopWords = stopwords.words("english")

    # 清除文章内和type相关的数据
    capital_type_list = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',
                        'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']
    # 大写和小写数据都进行处理
    lower_type_list = [x.lower() for x in capital_type_list]

    list_personality = []
    list_posts = []
    len_data = len(data)
    i = 0
	
    # 遍历每一行样本数据
    for row in data.iterrows():
        i += 1
        if (i % 500 == 0 or i == 1 or i == len_data):
            print("%s of %s rows" % (i, len_data))

        # 移除链接和无用注释等信息
        posts = row[1].posts
        temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', posts)
        temp = re.sub("[^a-zA-Z]", " ", temp)
        temp = re.sub(' +', ' ', temp).lower()

        # 去除停用词
        temp = " ".join([w for w in temp.split(' ') if w not in cachedStopWords])

        # 去除type相关特征数据
        for index in range(len(capital_type_list)):
            temp = temp.replace(lower_type_list[index], "")

        # 将特征标签提取出来
        type_labelized = change_personality(row[1].type)
        list_personality.append(type_labelized)
        list_posts.append(temp)
```

在对posts的数据进行过滤清洗的过程中，我们把type也同时提取出来。因为题目给的8种人格中，其实是分为4组的，比如`Introversion (I) – Extroversion (E)`他们是反义词的关系，人只会具有其中的一种人格。因此我们可以把他们分别用0和1进行标记，比如一个`ISFP`的人格我们可以用`[0, 1, 0, 1]`来代表它。

我们将预处理数据的结果写入文件中，csv文件的格式如下：

```
0 0 0 0,   moment sportscenter top ten play prank life changing experience life repeat today may perc experience immerse last thing  friend posted facebook committing suicide next day rest peace hello  sorry hear distress natural relationship perfection time...
```



#### 2. 数据观察

我们这里有8种不同的人格，如果直接使用决策树对8中人格划分，如果要考虑不同组人格之间的相关性，一种直接的方法就是将各种不同的组人格组合在一起，这样就得到了16种不同的人格组合。就拿`INTJ`的人格来说，如果数据集中是`INTJ`的就标记为1，否则为0。然后我们就可以使用模型去分类了，不过这样模型的数据绝大多数都是0，少量为1。而且这样划分至少需要15个分类器，效率肯定也不好。

因此我们对4组人格之间的相关性进行了计算，直接用协方差计算即可。我们可以得到他们之间的相关系数如下：

|      | IE    | NS    | TF    | JP    |
| ---- | ----- | ----- | ----- | ----- |
| IE   | 1     | -0.04 | 0.07  | 0.16  |
| NS   | -0.04 | 1     | -0.01 | 0.01  |
| TF   | -0.06 | -0.08 | 1     | -0.01 |
| JP   | 0.16  | 0.01  | -0.01 | 1     |

观察数据可以发现，他们之间的相关性其实是非常低的，也就是I的人格对于它是N还是S，其实几乎是没有很大的关联。因此我们没有必要将他们构成特定的组合然后进行的决策。我们直接构建4个分类器分别对4组人格进行分类即可。



#### 3. 文本向量化

因为我们不可能把文本作为模型的输入，我们在训练模型的时候肯定只能输入一些量化的数值。在去年的自然语言处理课上，我们学到了很多把文本向量化的方法，比如词嵌入，word2vec，已经TFIDF等。我们这里使用的是TFIDF的方法对文本进行向量化。

我们这里使用了` sklearn.feature_extraction.text`库对数据进行TFIDF转化。

我们首先用`sklearn.feature_extraction.text`的`CountVectorizer`对文本的词语频率等信息进行统计

```PYTHON
# 用TFIDF的方法将用户post数据变成向量
def tfidf():
    # 从预处理数据中读取 并组成nparray
    list_posts, list_personality = read_preprocessing()

    # Posts to a matrix of token counts
    cntizer = CountVectorizer(analyzer="word",
                                 max_features=2000,  # 限制词典维度上限
                                 max_df=0.8,  # 在0.7以上的文章出现过的词语可以去除
                                 min_df=0.05) # 在0.05以下的文章出现过的词语可以去除

    # 计算词语频率
    print("CountVectorizer...")
    word_count = cntizer.fit_transform(list_posts)
    print(word_count)


    print("Tf-idf...")
    # 根据词语频率得到tfidf矩阵
    tfizer = TfidfTransformer()
    tfidf =  tfizer.fit_transform(word_count).toarray()
    print(tfidf, len(tfidf),len(tfidf[0]))

    return tfidf, list_personality
```



#### 4. 随机森林模型

按照上面所描述的方法我们分步骤构建随机森林模型。

##### 1 ）划分连续数据

首先词向量是一个连续的特征值，因此我们需要对它进行一个划分。在决策树的构建中，我们可以采用二分法去尝试找到分解连续变量的方法。在本次实验中，我们使用人工设定阈值来进行划分。我们将在阈值内的连续值用离散的值去代替。



##### 2）Gini系数选取特征

接着我们需要一个对特征进行选择的函数，我们选择使用基尼指数来进行特征的选择：

Gini指数其实就是：
$$
gini(D,A)=\sum_{j=1}^vp(A_j)\times gini(D_j|A=A_j)
$$
其中：
$$
gini(D_j|A= A_j) = \sum^{n}_{i-1}p_i(1-p_i)=1-\sum_{i=1}^np^2_i
$$

```python
def spilt_loss(left, right, class_values):
    loss = 0.0
    for class_value in class_values:
        left_size = len(left)
        if left_size != 0:
            prop = [row[-1] for row in left].count(class_value) / float(left_size)
            loss += (prop * (1.0 - prop))
        right_size = len(right)
        if right_size != 0:
            prop = [row[-1] for row in right].count(class_value) / float(right_size)
            loss += (prop * (1.0 - prop))
    return loss
```



##### 3）随机选取数据和特征

这是随机森林和决策树的主要区别，我们人工设定一定的比例拿去数据

```python
# 随机选取每个树的训练样本
train = get_subsample(train, ratio)

# 随机选取每个数的训练特征
while len(features) < n_features:
    index = randrange(len(dataSet[0]) - 1)
    if index not in features:
        features.append(index)
```



##### 4）递归构造决策树

```python
# 从root节点递归构建决策树
def build_tree(dataSet, n_features, max_depth, min_size):
    root = get_best_spilt(dataSet, n_features)
    sub_spilt(root, n_features, max_depth, min_size, 1)
    return root
```



##### 5）设定参数构造随机森林

```PYTHON
# 创建随机森林
def random_forest(train, test, ratio, n_feature, max_depth, min_size, n_trees):
    trees = []
    for i in range(n_trees):
        train = get_subsample(train, ratio)   #从切割的数据集中选取子集
        tree = build_tree(train, n_features, max_depth, min_size)  # 构建决策树
        trees.append(tree)
    predict_values = [bagging_predict(trees, row) for row in test]
    return predict_values
```





#### 5. 实验结果及模型对比

我们将随机森林的实验结果和`xgboost`的实验结果进行了对比。

```python
def train_models(tfidf, list_personality):
    X = tfidf

    for l in range(len(type_indicators)):
        print("%s ..." % l)

        Y = list_personality[:, l]

        seed = 7
        test_size = 0.33
        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)

        # fit model on training data
        model = XGBClassifier()
        # model = RandomForestClassifier(max_depth = 2)
        model.fit(X_train, y_train)

        # make predictions for test data
        y_pred = model.predict(X_test)
        predictions = [round(value) for value in y_pred]
        # evaluate predictions
        accuracy = accuracy_score(y_test, predictions)
```

我们分别对4组人格进行分类，然后分别测试他们在测试集上的正确率：

|                | IE     | NS     | FT     | JP     |
| -------------- | ------ | ------ | ------ | ------ |
| Xgboost        | 78.23% | 86.06% | 73.04% | 64.83% |
| RandomForecast | 77.86% | 86.03% | 68.51% | 60.92% |
| DecisionTree   | 71.65% | 85.99% | 60.81% | 63.88% |

- 对比可以发现，`Xgboost`在所有人格上，都取得了更优秀的分类正确率。这可能是因为`xgboost`在代价函数中加入了正则项用来控制模型的复杂度，这样学习出来的模型更简单，也可以防止过拟合。

  不过他们其实正确率并没有很大的区别，这里面存在一些偶然的因素，比如训练集是否足够量，测试集的划分是否刚好对于Xgboost模型训练结果更有利等因素，或者是数据处理过程中是否有不妥当的地方。这些都是导师它们准确率产生差距的原因，并不能一口咬定Xgboost在分类任务上就会比随机森林算法效果更好。

- 我同时使用了决策树模型来进行预测，结果发现决策树的效果并没有比随机森林差很多，甚至在第四组特征的时候还有优于随机森林的结果。不过这和测试集的划分有关系，而且对于参数的调整也有一定的关系。



上面是对单个人格进行预测的结果，可以看到准确率基本可以达到了70以上，不过如果想要四个同时预测准确，正确率将会非常低：

|                | 4组特征同时预测正确 |
| -------------- | ------------------- |
| Xgboost        | 35.23%              |
| RandomForecast | 31.12%              |

这个结果和训练集和测试集的划分情况有很大的关系，这里取的结果是相对常见的结果，在尝试过程中，也出现正确率只有20%左右，也出现过40%以上的。