## 智能算法与应用作者报告

#### 摘要

本文对TSP问题的尝试求解，设计并对比了三种不同的算法策略，分别为：局部搜索策略，模拟退火策略和遗传算法。首先从局部搜索策略开始，之后改进加入随机性的模拟退火策略来弥补局部搜索的不足。最后还使用了遗传算法，通过尝试不同的交叉策略来优化TSP问题尝试找到最优解。

经过实验看来，模拟退火方法取得了比较好的效果，最优的一次误差仅不足2%。而遗传算法稍逊于模拟退火算法，误差率在5%左右。最差的是局部搜索策略，有时候会在误差高达80%的时候就收敛，这也是因为没有随机性导致在局部最优处收敛而出现的。



#### 一、 导言

本实验处理的问题是著名的旅行商问题，问题大致为：地图上有多个城市，它们之间存在距离，一个人希望经过所有的城市，且每个城市只能经过一次，最后再回到出发点，我们如何设计一条行进路线，让总的距离最短。

本实验采用的数据集是著名的TSPLIB，我们在里面选取了数据，数据集包含两个文件，一个是城市的index和对应地图坐标，第二个文件包含最优的路径。通过坐标我们可以得到城市之间的距离，实验的目标是找到一条路径相对最优的路径尽量去接近数据集提供的最优解。

实验中尝试了多种不同的算法，从基础的局部搜索算法，进而加入模拟退火策略进行对比，还尝试遗传算法来求解问题。实验最后对对比三种不同的算法并得出一些经验和结论。



#### 二、 实验过程

##### 1. 局部搜索 local search

###### 1.1 算法原理

局部搜索可以理解为一个最基础的贪心算法。该算法每次从当前解的临近解空间选择一个最优解作为当前解，直到达到一个局部最优解。不同的局部搜索算法的区别就在于如何去选择临近解空间，这个选择方法的设计很大程度决定了局部搜索算法的准确度上限。

局部搜索算法的思想很单纯，但是也存在一个很大的缺陷，就是它很容易陷入一个局部最优解，而这个局部最优解往往都不是全局的最优解。在神经网络里求最优解的梯度下降法也存在这样的问题，但是它往往要求目标函数是一个存在唯一最优解的情况。

<img src="assets/智能算法与应用作者报告 实验一/image-20200721162323881.png" alt="image-20200721162323881" style="zoom:67%;" />



###### 1.2 数据处理与一些准备

首先我们需要对城市坐标数据和最优路径数据进行读取。我们选取数据集 `kroC100`来进行实验。网站上包含两个文件，分别是城市坐标数据 `kroC100.tsp` 和最优解的路线 `kroC100.opt.tour` 。编写 `read_data`函数对读取城市的坐标和最短的路径。坐标数据格式大致如下：

````txt
kroC100.tsp:
1 1 1357 1905
2 2650 802
3 1774 107
4 1307 964
5 3806 746
6 2687 1353
````

为了提升训练速度，我们提前将所有城市之间的距离计算出来，并保存在矩阵中：

````python
def cal_dist_mat(N):
    """
        计算距离矩阵
    """
    dist_mat = np.zeros((N,N))
    for i in range(N):
        for j in range(i,N):
            dist_mat[i][j] = dist_mat[j][i] = np.linalg.norm(cities[i]-cities[j])
    return dist_mat
dist_mat = cal_dist_mat(N)
````

同时我们可以编写一些小的工具函数帮助我们简化算法的编写：

首先我们需要一个函数来计算某一个路径所需要的路程总和：

````python
def cal_path_len(path):
    """
        输入路径
        输出路径对应的距离
    """
    path_len = 0
    for i in range(N-1):
        path_len += dist_mat[path[i]][path[i+1]] 
    path_len += dist_mat[path[N-1]][path[0]]
    return path_len
````

接着我们需要一个小函数判断两个路径是否完全相同。如果在迭代的过程中发现两个路径完全相同，我们就需要停止迭代过程



###### 1.3 选择临近解空间

对于TSP问题，最常见的三种临近解的选择方法有一下三种：

###### 1.3.1 随机交换两个城市顺序

````python
def change_city_pos(path):
    """
        随机交换两个城市在路径中的位置, 得到新路径
    """
    while True:
        point1 = np.random.randint(0, N-1)
        point2 = np.random.randint(0, N-1)
        if point1 != point2:
            break
    path[point1], path[point2] = path[point2], path[point1]
    return path
````

随机交换两个点在路径上的位置，得到更新后的路径



###### 1.3.2 随机反转路径

````python
def random_reverse_subpath(path):
    """
        随机将原路径中的部分路径反向
    """
    while True:
        point1 = np.random.randint(0, N-1)
        point2 = np.random.randint(0, N-1)
        if point1 != point2 and point1 < point2:
            break
    path[point1:point2+1] = path[point1:point2+1][::-1]
    return path
````

在原始路径上随机选择两个城市，然后调换它们之间的路径的方向，举例说明：

````
# 原始路径为：1，2，3，4，5，6，7，8
# 抽取的点位置为2，5
# 调换第二个和第五个点之间的路径，得到新的路径
# 1，5，4，3，2，7，8
````



###### 1.3.3 随机交换两段路径

````python
def exchange_subpath(path):
    """
        交换两段路径得到新的路径
    """
    while True:    
        lst = [np.random.randint(0, N-1) for i in range(4)]
        set_lst = set(lst)
        if len(set_lst) == len(lst):
            lst.sort()
            break
    path1 = path[:lst[0]]
    path2 = path[lst[0]:lst[1]]
    path3 = path[lst[1]:lst[2]]
    path4 = path[lst[2]:lst[3]]
    path5 = path[lst[3]:]

    path = np.append(path1, path4)
    path = np.append(path, path3)
    path = np.append(path, path2)
    path = np.append(path,path5)
    return path
````

从城市中随机选择4个点1，2，3，4，然后交换1，2之间的路径和3，4之间的路径，比如：

````
1,2,3,4,5,6,7,8,9,0
# 随机选择点为2 4 6 8交换后为：
1，2，7，8，5，6，3，4，0
````



###### 1.4 迭代局部搜索算法

接着我们需要设计迭代的搜索方法对TSP问题进行局部搜索。我们首先随机初始化一个路径，接着我们从这个随机路径开始进行局部搜索。初始的随机路径很大程度决定了我们的算法是否能到达或逼近全局的最优解。

````python
# 随机选取一个相对较短的路径作为初始路径
path = np.arange(N)
shuffle(path)
path_len = cal_path_len(path)
````

假定我们每轮迭代10*N次，也就是用选择的策略找出10N个路径，并从中选出长度最短的一条作为本轮迭代的最优解，不断重复，直到解在相邻两轮内不会变化，也就是路径在局部达到了相对最优的情况，则此时停止迭代：

````python
    while True:
        iteration += 1
        paths = [best_path]
        # 随机尝试10*N次的临域解
        for i in range(10*N):
            tmp = path.copy()
            tmp = strategy(tmp)
            paths.append(tmp)
        # 选取本轮迭代距离最短路径作为本轮最优结果
        best_path_len, best_path = get_best_path(paths)
        draw(best_path, best_path_len)
        path_lens.append(best_path_len)
        if compare(best_path, path):
            break 
        path = best_path.copy()
        print("iteration:", iteration)
    opt_dist = cal_path_len(best_path)
    actual_opt_dist = cal_path_len(shortest_path)
````

最后统计输出数据并绘制图像，结论在下一节中展示。



###### 1.5 局部搜索实验结果

经过实验可以发现不同的临近解选取方法之间，存在的很大的差异。本人运行几次后并选取相对最优的情况展示如下：

| 临近解选法             | 局部最优路径长度 | 超出最优情况比例 |
| ---------------------- | ---------------- | ---------------- |
| change_city_pos        | 32450            | 56.3%            |
| random_reverse_subpath | 21900            | 5.54%            |
| exchange_subpath       | 36379            | 75.3%            |

可以看到，`random_reverse_subpath`方法比起另外两种方法准确度高很多，可见它的临近解的选取策略相对另外两者更优。

下图展示了`random_reverse_subpath` 变化情况， 最开始的时候随机生成的路径用图像展示为：

<img src="assets/智能算法与应用作者报告 实验一/image-20200722002628456.png" alt="image-20200722002628456" style="zoom:40%;" />

可以看到图像显然不可能是最优的情况，路线有很多交叉。

经过101轮的迭代后，算法收敛到了局部最优点，我们可以看到路径显然简化了很多，相互交叉的路径数量大大减少，和最优情况也相对比较接近。

<img src="assets/智能算法与应用作者报告 实验一/image-20200722002807771.png" alt="image-20200722002807771" style="zoom:40%;" />

对距离和迭代轮次进行绘图，我们可以看到迭代过程中，距离越来越短，不断缩小，最后在局部最优解收敛：

<img src="assets/智能算法与应用作者报告 实验一/image-20200722002900921.png" alt="image-20200722002900921" style="zoom:40%;" />



##### 2. 模拟退火算法

###### 2.1 算法原理

局部搜索算法是完完全全的贪心算法，正如它名字所说的，这种贪心鼠目寸光，只把目标放在局部最优解上，因此只能搜索到局部的最优值。

模拟退火算法来源于固体退火原理。在生活中固体加热后物理退火的过程中，材料中的原子原来会停留在使内能有局部最小值的位置，加热使能量变大，原子会离开原来位置，而随机在其他位置中移动。退火冷却时速度较慢，使得原子有较多可能可以找到内能比原先更低的位置。

模拟退火: 其原理也和固体退火的原理近似。模拟退火算法从某一较高初温出发，伴随温度参数的不断下降，结合概率突跳特性在解空间中随机寻找目标函数的全局最优解，即在局部最优解能概率性地跳出并最终趋于全局最优。

根据热力学原理，在温度为T的时候，出现能量差为$\delta E$的概率为$P(\delta E)$，表示为：
$$
P(\delta E)=e^{\frac{\delta E}{KT}}
$$
其中K是一个常数。$\delta E$总是小于0的。因此这条公式指出：

1. 温度越高，出现一次能量差为$\delta E$的降温概率就越大。
2. 温度越低，则出现降温的概率就越小。

随着温度T的降低，$P(\delta E)$会越来越小。我们将一次向较差解的移动看作是一次温度跳变的过程，以概率$P(\delta E)$来接受它。还是用这个图来分析：

<img src="assets/智能算法与应用作者报告 实验一/image-20200721162323881.png" alt="image-20200721162323881" style="zoom:67%;" />

从B移向BC之间的小波峰D时，**每次右移(即接受一个更糟糕值)的概率在逐渐降低。**如果这个坡特别长，那么很有可能最终我们并不会翻过这个坡。如果它不太长，这很有可能会翻过它，取决于衰减 t 值的设定。

模拟退火的基本流程如下：

<img src="assets/智能算法与应用作者报告 实验一/image-20200722115844135.png" alt="image-20200722115844135" style="zoom: 50%;" />

算法包括几个基本的要素：

1. 设定初始温度：初始温度越大，获得高质量的解的几率就越大，但计算时间就越长（降温时间变长）。

2. 邻域函数：选择合适的领域函数获取临近解，这里复用局部搜索设计的三个函数。

3. 接受概率：从一个可行解到另一个可行解转移的概率，一般用Metropolis准则
   $$
   P_{ij}^T=
   \left\{
   \begin{aligned}
   &1		&	&,if\ E(j)\le E(i)\\
   &e^{-\frac{E(j)-E(i)}{KT}}=e^{-(\frac{\Delta E}{KT})}	&	& ,otherwise
   \end{aligned}
   \right.
   $$

4. 冷却控制：从某一较高温度向较低温度降低的规则。

5. 内层平衡：用于决定在各温度下产生候选解的数目

6. 终止条件：算法的终止条件



###### 2.2 确定参数和初始化

我们首先需要设计上面6个基本要素。

- 初始问题我们设定为5000度，随机选取一个相对较短的路径作为初始路径。
- 领域函数使用第一个实验中效果最好的二号操作方法。
- 接受概率就使用Metropolis准则。
- 冷却控制我们使用一个下降系数q=0.98，每轮迭代后温度变为原来的0.98倍。
- 内层平衡我们设定每一轮降温进行L=1000次的新解。
- 终止条件为温度小于T_end = 0.01的时候停止

编写代码如下：

````python
cur_path = np.arange(N)
shuffle(cur_path)
cur_path_len = cal_path_len(cur_path)

all_path_len = [cur_path_len]   # 记录每轮降温的温度变化，用于绘图

T = 5000 # 默认初始温度为度
T_end = 0.01   # 终止条件为0.01度
q = 0.98    # 退火系数
L = 1000    # 每隔温度迭代1000次 
strategy = random_reverse_subpath   # 默认使用第二种临近解方法
````



###### 2.3 模拟退火算法实现

根据上面设定的参数，我们设计模拟退火算法如下：

````python
while(T>T_end):
    print("current T：",T)
    for i in range(L):
        tmp_path = strategy(cur_path)
        tmp_len = cal_path_len(tmp_path)
        dE = tmp_len - cur_path_len
        if dE < 0:  # 如果新解更优，直接获取它
            cur_path = tmp_path.copy()
            cur_path_len = tmp_len
            else:   # 如果新解比较差，使用MetroPolis准则
                if np.random.rand() <= np.exp(-dE/T):   # 使用这个解
                    cur_path = tmp_path.copy()
                    cur_path_len = tmp_len
                    T = T*q # 退火
                    all_path_len.append(cur_path_len)  # 记录每轮降温的温度变化，用于绘图
````

每一轮降温中迭代1000次，每次都生成一条新的临域路径，如果更优就直接选择，如果不优就用metropolis方法一定概率去接受。



###### 2.4 模拟退火实验结果

模拟退火算法实验结果远远优于一般的局部搜索算法，我们这里使用的参数为：

````python
T = 5000 # 默认初始温度为度
T_end = 0.01   # 终止条件为0.01度
q = 0.98    # 退火系数
L = 1000    # 每隔温度迭代1000次
strategy = random_reverse_subpath   # 默认使用第二种临近解方法
````

默认使用第二种临近解的获取方法。我们可以得到以下的实验结果：

|      | 局部最优路径长度 | 超出最优路径比例 |
| ---- | ---------------- | ---------------- |
|      | 12088.83         | 1.63%            |

可以看到最优解的路径图为：

<img src="assets/智能算法与应用作者报告 实验一/image-20200722132724043.png" alt="image-20200722132724043" style="zoom:50%;" />

路径距离随着温度降低的变化图为：

<img src="assets/智能算法与应用作者报告 实验一/image-20200722132803404.png" alt="image-20200722132803404" style="zoom:50%;" />

可以看到，曲线并不是一条平滑下降的曲线，而是会存在上下波动的，在温度较高的时候，路径长度经常会出现反向上升的情况，这是因为扰动导致的，而这也使我们的算法能有效的逼近全局最优解。因此误差率仅为不到2%，远优于一般的局部搜索算法。



##### 3. 遗传算法

###### 3.1 算法原理

遗传算法是模仿自然界生物进化机制发展起来的随机全局搜索和优化方法，它借鉴了达尔文的进化论和孟德尔的遗传学说。其本质是一种高效、并行、全局搜索的方法，它能在搜索过程中自动获取和积累有关搜索空间的知识，并自适应的控制搜索过程以求得最优解。

遗传算法操作使用适者生存的原则，在潜在的解决方案种群中逐次产生一个近似最优解的方案，在遗传算法的每一代中，根据个体在问题域中的适应度值和从自然遗传学中借鉴来的再造方法进行个体选择，产生一个新的近似解。这个过程导致种群中个体的进化，得到的新个体比原来个体更能适应环境，就像自然界中的改造一样。

遗传算法的基本流程如下：

<img src="assets/智能算法与应用作者报告 实验一/image-20200722141133659.png" alt="image-20200722141133659" style="zoom:50%;" />

在解决TSP问题中，遗传算法的步骤大致如下：

1. 初始化：生成一定规模的种群，随机生成。
2. 评估。通过某种方法来评估个体的适应度。个体的生存能力。
3. 选择。类似于自然选择，优良的基因、生存能力强的被选择下来的概率要大
4. 交叉。产生后代，基因交叉
5. 变异。后代的基因可能会变异



###### 3.2 算法实现

我们首先定义遗传算法的参数，并随机生成一定规模的种群

````python
# 种群数
pop_count = 150
# 进化次数
iteration = 2000
# 交配概率
crossover_rate = 0.8
# 变异率
mutation_rate = 0.2
# 适者生存的比例
retain_rate = 0.3
# 弱者被接受的概率
random_select_rate = 0.3

population = []
for i in range(pop_count):
    x = [i for i in range(N)]
    shuffle(x)
    population.append(x)
````

###### 3.2.1 适者生存

接着我们需要实现适者生存的规则，我们对种群中所有的可能路径按照长度排序。选取长度最短的前几条作为一定会生存下来的强者，我们用一个比例来代表：`retain_rate`。对于不够强大的个体，我们有一定的概率去接受它，假定这个概率是 `random_select_rate`，于是我们可以编写代码：

````python
def selection(population):
    """
        自然选择, 适应性强的生存，适应性弱的有一定几率生存，保证种群数量不变
    """
    population = sorted(population, key = cal_path_len)

    # 适应能力强的比例为
    retain_length = int(len(population) * sustain_rate)
    sustain = population[:retain_length]    # 适应能力强，一定会存活下来

    # 适应能力不够强，但有一定几率可以存活
    for chromosome in population[retain_length:]:
        if random.random() < random_select_rate:
            sustain.append(chromosome)

    # 用适应性强的个体补充数量，保证种群数量不变
    if len(sustain) < pop_count:
        sustain = sustain + population[:pop_count-len(sustain)]
    return sustaina
````

要注意我们要保持原来的种群大小不变，但是经过淘汰后一定会有一部分个体被淘汰，因此我们需要用强者来补齐种群数量。

###### 3.2.2 变异

我们采用局部搜索策略来代替变异过程：

````python
def mutation(population):
    """
        发生变异，用临近解选择策略重新生成路径
    """
    for i,path in enumerate(population):
        if np.random.rand() < mutation_rate:
            tmp_path = random_reverse_subpath(path) # 发生变异，用临近策略重新生成
            population[i] = tmp_path
    return population
````

这里选取的局部搜索策略也是在局部搜索实验中确定效果最好的一个策略。



###### 3.2.3 杂交

对于路径有可能出现杂交现象。这里设计的杂交策略是，对于一个种群，对相邻的两个路径进行一定概率杂交。比如路径A和路径B，我们随机生成一个区间[left， right]，然后我们交换A和B在left到right区间内的城市。

````python
def cross(population):
    """
        遗传算法交配策略
        相邻的两条路径有一定几率发生交配
    """
    index = 0
    while index < pop_count-1:
        if random.random() < crossover_rate:
            index += 1
            continue
        else:
            while True:
                left=random.randint(0,N)
                right=random.randint(0,N)
                if left != right and left < right:
                    break
            pop1, pop2 = population[index], population[index+1]

            # 交叉片段，因此gene1和gene2不完全相同，要把各自缺少的城市补上
            gene1 = pop1[left:right]
            gene2 = pop2[left:right]
            p1_other, p2_other = [], []
            for i in range(len(pop1)):
                if pop1[i] not in gene2:
                    p1_other.append(pop1[i])
                if pop2[i] not in gene1:
                    p2_other.append(pop2[i])
            population[index] = p2_other[:left] + gene1 + p2_other[left:]
            population[index+1] = p1_other[:left] + gene2 + p1_other[left:]
            index += 2
    return population
````

这里要注意，因为A和B在这个区间内的城市并不是相同的，如果直接交换可能导致有的城市无法前往，有的城市重复前往，因此i我们需要最后做一个兜底的去重工作：



###### 3.3 实验结果

遗传算法的结果和模拟退火的实验效果想近，比一般的局部搜索算法效果更好，经过几次实验的尝试，我们得到以下的实验结果：

| 局部最优路径长度 | 超出全局最优路径长度比例 | 训练耗时 |
| ---------------- | ------------------------ | -------- |
| 21989.1          | 5.82%                    | 84s      |

训练出来的TSP路线图如下：

<img src="assets/智能算法与应用作者报告 实验一/image-20200722163047541.png" alt="image-20200722163047541" style="zoom:50%;" />

可以看到距离变化的曲线如下：

<img src="assets/智能算法与应用作者报告 实验一/image-20200722163025974.png" alt="image-20200722163025974" style="zoom:50%;" />

#### 三、结果分析

因为报告较长，为方便查看，三个实验的结果图已经融入在实验过程中。本节仅做对比分析：

##### 1. 局部搜索和模拟退火算法的对比

不管从理论上还是从实验上都可以看出，模拟退火的实验效果是远远优于局部搜索的。因为模拟退火存在一定的随机性，通过分析距离随迭代变化曲线就可以看出来区别。局部搜索策略是一条光滑的下降的曲线，而模拟退火是会上下抖动的。因此模拟退火算法是存在可能跳出局部最优，去逼近全局最优的情况的。而且因为加入了温度因素，到后期温度下降后，抖动的频率就会比较低，最后慢慢的收敛。

不过局部搜索策略的优势在于它的训练速度很快，这是因为它很快就会达到收敛，陷入局部最优。而模拟退火因为随机性存在，需要更大量的迭代次数才能达到比较好的效果。

##### 2. 模拟退火和遗传算法对比

从实验结论可以看出，在这个TSP问题中，模拟退火的实验效果比遗传算法更好。

模拟退火算法的优点在于，它的随机性可以帮助他跳出局部最优解以达到全局最优解。而这个随机性的开销是比较小的，因此模拟退火算法运行效率更快。而遗传算法运行速度就相对较慢，因为它每一轮迭代需要做一些交换、淘汰的操作，需要遍历，因此耗时更长，不过从拓展性上看，因为遗传算法是一个群体的算法，如果采用并行的运算肯定可以大幅度提升它的运行效率。

遗传算法的优点是，随机性更强，能够很好的处理约束，更容易跳出局部最优去找到全局最优解。但是它需要对参数细致调节，对交换策略等都需要有更好的设计，因此在本次实验中，可能是对参数缺乏更细致的调节，各种遗传策略还有更好的设计，因此它的效果并没有赶上模拟退火的效果。



#### 四、结论

##### 1. 设计高效遗传算法的一些经验

设计遗传算法其实就是按照遗传算法要素进行：

1. 初始化：在本次实验中，对于初始化都是直接随机生成城市序列。但是事实上，初始的情况对于最终能否达到全局最优解有着很重要的意义。因此在初始化的时候，我们可以加入一些策略，比如贪心策略，迭代100次，选取最好的一个作为初始情况。或者对实际问题，人为的设计一些可能接近最优情况的方法。比如TSP问题，我们就可以绘制散点图，然后尽量少的让路线交叉，绘制一条路径，这样更容易收敛到全局的最优情况。

2. 选择：接着我们需要设计适者生存的条件。这个其实是遗传算法的误差能不断下降收敛的关键原因。我们通过一个概率值来控制被淘汰的个体有多大的概率活下来，事实上，最优解的基因很有可能是存在在被淘汰的个体上的，因此如何控制概率，能让我们的误差一直下降，但是又能更好的到达全局最优解，需要做一些尝试。

   我们可以对每个参数跑10次算法，每次算法都从固定的初始值开始，最后取平均值，然后比较哪一个参数更适合当前问题。

3. 交叉和变异：遗传算法的随机性的重要一点就在于它会进行交叉和变异。对于交叉我们模仿基因的交叉杂交来设计，也可以有很多别的设计方法，比如，让差的和好的进行交换，更有可能让保存下来的个体中包含随机性的基因。而突变这里使用了局部搜索策略，当然也可以使用别的突变方式，比如随机交换两个城市等。这需要尝试和一些设计，才能得到比较好的效果。

对于想要高效的运行遗传算法，遗传算法其实具备天然的并行性，因为它是一个种群的操作，而种群的个体是平等的，所以可以把进程分配到多台机器并行处理种群的变异操作。现在著名的并行遗传算法模型包括：

1. 主从式模型
2. 粗\细粒度模型
3. 混合模型

而对于一些实际问题，比如这里的TSP问题，我们可以对城市进行编码来进行运算，效率比起用字符串会更快。而在一些实际问题中，我们甚至可以模仿人体的基因去二进制编码，用01串来进行位运算一定会比整形类型的运算效率快很多。



##### 2. 比较单点搜索和多点搜索的优缺点

单点搜索其实就是我们上面的局部搜索策略，它很容易陷入一个局部的最优值。虽然通过加入随机性因素有可能让他去找到全局的最优值，但是还是很大程度局限于初始点的选择。而多点搜索就是为了改进它这个不足，从多个不同的点出发开始搜索，而且可以改造成并行的运算，多台机器从多个不同的起点出发解决同一个问题。但是它比起淡点搜索速度肯定会比较慢，消耗的资源更多。