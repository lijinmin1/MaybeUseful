## 人工智能

1. 信息熵：

   信息熵的公式为：
   $$
   H(x)=-\sum_{x\in X}p_ilog_2(p_i)=E(log_2 \frac{1}{p(x)})
   $$
   信息熵越大，不确定性就越大

   信息增益：$g(Y,X)=H(Y)-H(Y|X)$

   Y的不确定性减去引入X后Y的不确定性，差值越大可见X对Y的不确定性影响较大。

   **ID3模型**就是基于信息增益得到的，选取信息增益最大的节点

   **C4.5模型**

   信息增益偏向分支数多的节点（考虑极端情况每个节点都只有一个数据）

   因此回除以一个归一化的量

2. 启发式搜索：

   定义评价函数$f(n)=g(n)+h(n)$

   可采纳性：对每个点预测到达终点的成本必须小于实际的成本，终点要为0

   一致性：估计的值在接近终点时需要越来越接近实际的路径长度$h(n_1)-h(n_2)<=c(n_1\rightarrow n_2)$

3. 8数码问题：

   利用松弛问题的方法。放松问题的条件来构造启发式函数：

   比如这个松弛方法：只要A和B相邻就可以移动过去。可以想到这样的启发式函数：因此令h(n)=所有方块到达其目标位置的曼哈顿距离之和。

4. 博弈树搜索

   根据玩家轮流下棋的顺序，会有极大节点和极小节点，构建完博弈树后自底向上传播计算每个节点胜负关系，最后到根节点。

   <img src="assets/人工智能/IMG_2BB3FF32474E-1.jpeg" alt="IMG_2BB3FF32474E-1" style="zoom:50%;" />

5. 约束满足问题

   根据约束筛选可以选择的变量，减少需要筛选的空间。例如8皇后问题

6. 有5种袋子，糖果比例不同。你选择一个袋子，通过一直吃糖，预测当前吃的是哪个袋子。

   - 贝叶斯参数估计：

   $$
   p(h|d)=\frac{p(h,d)}{p(d)}=\frac{p(h,d)}{\sum_hp(h,d)}=\frac{p(d|h)\times p(h)}{\sum_hp(h,d)}
   $$

   

   **如果先验（每个袋子出现的可能性）给的合理**，贝叶斯一定是最优的，也不会存在过拟合问题。**但存在的问题是**，假如假设空间很大，有很多先验，无法叠加每种先验情况，因此无法用贝叶斯估计的方法

   - 极大后验概率：

   **如果不知道有多少种先验假设，没办法考虑所有的袋子然后求和，所以只能直接选择可能性最大的袋子来预测下一个糖果。**

   - 极大似然估计：

     ML也是取概率最大的，不过它连袋子的先验都不知道，就直接用d来计算最大概率的袋子